{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSoftmax(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def call(self, z, mask = None):\n",
    "        \n",
    "        z = tf.keras.backend.exp(z)\n",
    "        \n",
    "        if not mask is None:\n",
    "            assert(mask.shape == z.shape), 'Mask has incorrect dimensions: ' + str(z.shape) + ' vs. ' + str(mask.shape)\n",
    "            z = tf.multiply(z, tf.dtypes.cast(mask, 'float32'))\n",
    "        \n",
    "        return tf.divide(z, tf.reduce_sum(z, axis = -1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttn(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, h = 8, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.h = h\n",
    "        self.input_spec = tf.keras.layers.InputSpec(ndim = 3)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        assert(len(input_shape) == 3), 'Expected input shape of (m, Tx, d)'\n",
    "\n",
    "        (self.m, self.k, self.model_dim) = input_shape\n",
    "        \n",
    "        self.projected_dim = self.model_dim//self.h\n",
    "        \n",
    "        self.W1 = self.add_weight(\n",
    "                shape = (self.h, self.model_dim, self.projected_dim), \n",
    "                initializer = 'glorot_normal', \n",
    "                trainable = True)\n",
    "        \n",
    "        self.W2 = self.add_weight(\n",
    "            shape = (self.projected_dim * self.h, self.model_dim),\n",
    "            initializer= 'glorot_normal',\n",
    "            trainable = True)\n",
    "        \n",
    "    def call(self, X):\n",
    "        \n",
    "        X = tf.expand_dims(X, 1)\n",
    "        \n",
    "        projected = tf.matmul(X, self.W1)\n",
    "        \n",
    "        energies = tf.multiply(tf.matmul(projected,projected,transpose_b=True),1/self.projected_dim**0.5)\n",
    "        \n",
    "        alphas = tf.nn.softmax(energies, axis = -1)\n",
    "        \n",
    "        context = tf.reshape(energy, (self.m, self.k, -1))\n",
    "        \n",
    "        output = tf.matmul(tf.dtypes.cast(context, 'float32'), self.W2)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerNode(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, h = 8, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.h = h\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        (m, k, model_dim) = input_shape\n",
    "        \n",
    "        self.multihead_attn = MultiHeadAttn(self.h)\n",
    "        self.norm1 = tf.keras.layers.BatchNormalization()\n",
    "        self.dense1 = tf.keras.layers.Dense(model_dim, activation = 'relu', use_bias = True)\n",
    "        self.dense2 = tf.keras.layers.Dense(model_dim, use_bias = True)\n",
    "        self.norm2 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "    def call(self, X):\n",
    "        \n",
    "        X = self.multihead_attn(X) + X\n",
    "        \n",
    "        X = self.norm1(X)\n",
    "        \n",
    "        X_bypass = X\n",
    "        \n",
    "        X = self.dense1(X)\n",
    "        \n",
    "        X = self.dense2(X)\n",
    "        \n",
    "        X = X + X_bypass\n",
    "        \n",
    "        X = self.norm2(X)\n",
    "        \n",
    "        return X      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 5, 512])"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = tf.keras.layers.Embedding(10000, 512, mask_zero=True)\n",
    "attn = MultiHeadAttn(h = 8)\n",
    "trans = Transformer(8)\n",
    "\n",
    "X = np.array([[0,0,0,5,8],[1,0,0,3,3]])\n",
    "\n",
    "X = embed(X)\n",
    "X.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 5, 512])"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans(X).get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 8, 5, 64])"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.random.rand(8, 512, 64)\n",
    "Wo = np.random.rand(512,512)\n",
    "X = np.random.rand(2,1,5,512)\n",
    "\n",
    "proj = tf.matmul(X,W)\n",
    "proj.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 8, 5, 5])"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = tf.multiply(tf.matmul(proj,proj,transpose_b=True),1/64**0.5)\n",
    "alpha.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 8, 5, 5])"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_alpha = tf.nn.softmax(alpha_norm, axis = -1)\n",
    "soft_alpha.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 8, 5, 64])"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "energy = tf.matmul(soft_alpha, proj)\n",
    "energy.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 5, 512])"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = tf.reshape(energy, (2, 5, -1))\n",
    "c.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 5, 512])"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tf.matmul(c, Wo)\n",
    "output.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
