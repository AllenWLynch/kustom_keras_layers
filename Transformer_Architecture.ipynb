{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing MaskedSoftmax Layer:\n",
    "\n",
    "Computes a softmax matrix of size Tx * Tx for self attention or interconnected layers. In trailing mode, calculates such that each softmax output only attends to entry_j such that j <= i:\n",
    "\n",
    "$[[1 0 0 0 0],\n",
    " [1 1 0 0 0],\n",
    " [1 1 1 0 0],\n",
    " [1 1 1 1 0],\n",
    " [1 1 1 1 1]]$\n",
    " \n",
    "Also takes into account the mask produced by the embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSoftmax(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, trailing = False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.trailing = trailing\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        assert(len(input_shape) >= 3), 'Expect rank >3 input: (batch_size, Tx, d) or (batch_size, heads, Tx, d)'\n",
    "        assert(input_shape[-1] == input_shape[-2]),'Last two ranks must be symmetrical for attn mechanism'\n",
    "        \n",
    "        if self.trailing:\n",
    "            (Tx, d) = input_shape[-2:]\n",
    "\n",
    "            num_ones = 0.5 * (Tx**2 + Tx)\n",
    "\n",
    "            self.trailing_mask = tfp.math.fill_triangular(tf.ones(num_ones), upper = False)\n",
    "        \n",
    "    def call(self, z, mask = None):\n",
    "        \n",
    "        z = tf.exp(z)\n",
    "            \n",
    "        if self.trailing:\n",
    "            z = tf.multiply(z, self.trailing_mask)            \n",
    "            \n",
    "        z = tf.divide(z, tf.reduce_sum(z, axis = -1, keepdims=True))  \n",
    "        \n",
    "        #if not mask is None:\n",
    "        #    (m, Tx) = mask.get_shape()\n",
    "        #    mask = tf.reshape(mask, (m, 1, Tx, 1)) # creates shape (m, 1, 1, Tx) to be compatible with num heads\n",
    "        #    z = tf.multiply(z, tf.dtypes.cast(mask, 'float32'))\n",
    "            \n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 5])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.ones((2, 1, 5, 5))\n",
    "mask = tf.convert_to_tensor(np.array([[True, True, False, False, False], [True, True, True, True, False]]))\n",
    "mask.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = MaskedSoftmax(trailing = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=303, shape=(2, 1, 5, 5), dtype=float32, numpy=\n",
       "array([[[[1.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "         [0.5       , 0.5       , 0.        , 0.        , 0.        ],\n",
       "         [0.33333334, 0.33333334, 0.33333334, 0.        , 0.        ],\n",
       "         [0.25      , 0.25      , 0.25      , 0.25      , 0.        ],\n",
       "         [0.2       , 0.2       , 0.2       , 0.2       , 0.2       ]]],\n",
       "\n",
       "\n",
       "       [[[1.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "         [0.5       , 0.5       , 0.        , 0.        , 0.        ],\n",
       "         [0.33333334, 0.33333334, 0.33333334, 0.        , 0.        ],\n",
       "         [0.25      , 0.25      , 0.25      , 0.25      , 0.        ],\n",
       "         [0.2       , 0.2       , 0.2       , 0.2       , 0.2       ]]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t(X, mask = mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multihead Attention Layer\n",
    "\n",
    "1. Input is rank 3: (m, Tx, d_model)\n",
    "2. Project to h-dimensional space (h = num heads):\n",
    "<ol><li>a. expand dimension of X to be (m, 1, Tx, d_model)</li>\n",
    "    <li>b. $X \\in R^{m, 1, Tx, d_{model}} * W1 \\in R^{h, d_{model}, dv} = Proj \\in R^{m, h, Tx, dv}$</li>\n",
    "</ol><br>\n",
    "3. Calculate raw energies through dot product attention mechanism:<br><br>\n",
    "$ \\dfrac{1}{\\sqrt{len(dv)}} Proj\\cdot Proj^{T} = Energies \\in R^{m, h, Tx, Tx}$<br><br>\n",
    "4. Compute trailing (if decoder) or unmasked (if encoder) softmax to compute alphas:<br><br>\n",
    "$ Softmax(Energies) = Alphas $<br><br>\n",
    "5. Compute new context vectors through multiplication of Alphas (Query dot Key) and Values:<br><br>\n",
    "$ Alphas \\in R^{m, h, Tx, Tx} \\cdot Proj \\in R^{m, h, Tx, dv} = context \\in R^{m, h, Tx, dv} $<br>\n",
    "6. Stack context vectors from different heads back into original dimension:<br><br>\n",
    "$ Stack(context) = context \\in R^{m, Tx, h\\cdot dv} $ <br><br>\n",
    "7. Project back to model space:<br><br>\n",
    "$ context \\cdot W2 \\in R^{h\\cdot dv, d_{model}} = Y \\in R^{m, Tx, d_{model}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttn(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, heads = 8, encoder = True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.h = heads\n",
    "        self.input_spec = tf.keras.layers.InputSpec(ndim = 3)\n",
    "        self.encoder = encoder\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        assert(len(input_shape) == 3), 'Expected input shape of (m, Tx, d)'\n",
    "\n",
    "        (self.m, self.k, self.model_dim) = input_shape\n",
    "        \n",
    "        self.projected_dim = self.model_dim//self.h\n",
    "        \n",
    "        self.softmaxer = MaskedSoftmax(trailing = not self.encoder)\n",
    "        \n",
    "        self.reshaper = tf.keras.layers.Reshape((self.k, -1))\n",
    "        \n",
    "        self.W1 = self.add_weight(\n",
    "                shape = (self.h, self.model_dim, self.projected_dim), \n",
    "                initializer = 'glorot_normal', \n",
    "                trainable = True)\n",
    "        \n",
    "        self.W2 = self.add_weight(\n",
    "            shape = (self.projected_dim * self.h, self.model_dim),\n",
    "            initializer= 'glorot_normal',\n",
    "            trainable = True)\n",
    "        \n",
    "    def call(self, X, mask = None):\n",
    "        \n",
    "        X = tf.expand_dims(X, 1)\n",
    "        \n",
    "        projected = tf.matmul(X, self.W1)\n",
    "        \n",
    "        energies = tf.multiply(1/self.projected_dim**0.5,tf.matmul(projected,projected,transpose_b=True))\n",
    "        \n",
    "        alphas = self.softmaxer(energies, mask = mask)\n",
    "        \n",
    "        context = tf.matmul(alphas, projected)\n",
    "        \n",
    "        flattened = self.reshaper(context)\n",
    "        \n",
    "        output = tf.matmul(tf.dtypes.cast(flattened, 'float32'), self.W2)\n",
    "        \n",
    "        if not mask is None:\n",
    "            output = tf.multiply(output, tf.expand_dims(tf.dtypes.cast(mask, 'float32'), -1))\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = MultiHeadAttn(heads = 1, encoder = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([2, 5, 3]),\n",
       " <tf.Tensor: id=646, shape=(2, 5, 3), dtype=float32, numpy=\n",
       " array([[[ 0.3999936 , -1.092978  , -0.13726237],\n",
       "         [ 0.3999936 , -1.092978  , -0.13726237],\n",
       "         [ 0.        , -0.        , -0.        ],\n",
       "         [ 0.        , -0.        , -0.        ],\n",
       "         [ 0.        , -0.        , -0.        ]],\n",
       " \n",
       "        [[ 0.3999936 , -1.092978  , -0.13726237],\n",
       "         [ 0.3999936 , -1.092978  , -0.13726237],\n",
       "         [ 0.3999936 , -1.092978  , -0.13726236],\n",
       "         [ 0.3999936 , -1.092978  , -0.13726237],\n",
       "         [ 0.        , -0.        , -0.        ]]], dtype=float32)>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.ones((2,5,3))\n",
    "m = tf.convert_to_tensor(np.array([[True, True, False, False, False], [True, True, True, True, False]]))\n",
    "X = attn(X, mask = m)\n",
    "X.get_shape(), X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-Decoder Multihead\n",
    "\n",
    "1. Input is two matrices rank 3 (m, Tx, d_model): D, output from previous decoder layer; E, output from encoder stack\n",
    "2. Project E and D to h-dimensional space (h = num heads):\n",
    "<ol><li>a. expand dimension of E/D to be (m, 1, Tx, d_model)</li>\n",
    "    <li>b. $E/D \\in R^{m, 1, Tx, d_{model}} * W1 \\in R^{h, d_{model}, dv} = E/D_{proj} \\in R^{m, h, Tx, dv}$</li>\n",
    "</ol><br>\n",
    "3. Calculate raw energies through dot product attention mechanism, query = $D_{proj}$, key = $E_{proj}$:<br><br>\n",
    "$ \\dfrac{1}{\\sqrt{len(dv)}} D_{proj}\\cdot E_{proj}^{T} = Energies \\in R^{m, h, Tx, Tx}$<br><br>\n",
    "4. Compute softmax to compute alphas:<br><br>\n",
    "$ Softmax(Energies) = Alphas $<br><br>\n",
    "5. Compute new context vectors through multiplication of Alphas and Values (also $E_{proj}$):<br><br>\n",
    "$ Alphas \\in R^{m, h, Tx, Tx} \\cdot E_{proj} \\in R^{m, h, Tx, dv} = context \\in R^{m, h, Tx, dv} $<br>\n",
    "6. Stack context vectors from different heads back into original dimension:<br><br>\n",
    "$ Stack(context) = context \\in R^{m, Tx, h\\cdot dv} $ <br><br>\n",
    "7. Project back to model space:<br><br>\n",
    "$ context \\cdot W2 \\in R^{h\\cdot dv, d_{model}} = Y \\in R^{m, Tx, d_{model}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoderMultiHead(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, heads = 8, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.h = heads\n",
    "        self.input_spec = tf.keras.layers.InputSpec(ndim = 3)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        assert(type(input_shape) == list and len(input_shape) == 2),'Expected input as \"[encoder_seq, decoder_seq]\"'\n",
    "        assert(input_shape[0] == input_shape[1]), 'Expected encoder and decoder inputs to have same dimensions'\n",
    "\n",
    "        (self.m, self.k, self.model_dim) = input_shape[0]\n",
    "        \n",
    "        self.projected_dim = self.model_dim//self.h\n",
    "        \n",
    "        self.softmaxer = MaskedSoftmax(trailing = False)\n",
    "        \n",
    "        self.reshaper = tf.keras.layers.Reshape((self.k, -1))\n",
    "        \n",
    "        self.W1 = self.add_weight(\n",
    "                shape = (self.h, self.model_dim, self.projected_dim), \n",
    "                initializer = 'glorot_normal', \n",
    "                trainable = True)\n",
    "        \n",
    "        self.W2 = self.add_weight(\n",
    "            shape = (self.projected_dim * self.h, self.model_dim),\n",
    "            initializer= 'glorot_normal',\n",
    "            trainable = True)\n",
    "        \n",
    "    def call(self, inputs, mask = None):\n",
    "        \n",
    "        (encoder_in, decoder_in) = inputs\n",
    "        \n",
    "        encoder_in, decoder_in = tf.expand_dims(encoder_in, 1), tf.expand_dims(decoder_in, 1)\n",
    "        \n",
    "        encoder_proj, decoder_proj = tf.matmul(encoder_in, self.W1), tf.matmul(decoder_in, self.W1)\n",
    "        \n",
    "        energies = tf.multiply(1/self.projected_dim**0.5,tf.matmul(decoder_proj,encoder_proj,transpose_b=True))\n",
    "        \n",
    "        alphas = self.softmaxer(energies)\n",
    "        \n",
    "        context = tf.matmul(alphas, encoder_proj)\n",
    "        \n",
    "        flattened = self.reshaper(context)\n",
    "        \n",
    "        output = tf.matmul(tf.dtypes.cast(flattened, 'float32'), self.W2)\n",
    "        \n",
    "        if not mask is None:\n",
    "            output = tf.multiply(output, tf.expand_dims(tf.dtypes.cast(mask, 'float32'), -1))\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Layer\n",
    "\n",
    "1. $X = Normalize(Multihead(X) + X)$\n",
    "2. $X = Normalize(W2\\cdot relu(W1\\cdot X + b1) + b2 + X)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderNode(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, h = 8, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.h = h\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        assert(len(input_shape) == 3), 'Expected input of rank 3: (m, Tx, d)'\n",
    "        \n",
    "        (m, k, model_dim) = input_shape\n",
    "        \n",
    "        self.multihead_attn = MultiHeadAttn(self.h, encoder = True)\n",
    "        self.norm1 = tf.keras.layers.BatchNormalization()\n",
    "        self.norm2 = tf.keras.layers.BatchNormalization()\n",
    "        self.dense1 = tf.keras.layers.Dense(model_dim, activation = 'relu', use_bias = True)\n",
    "        self.dense2 = tf.keras.layers.Dense(model_dim, activation = 'linear', use_bias = True)\n",
    "        \n",
    "    def call(self, X, mask = None):\n",
    "                \n",
    "        X = self.norm1(self.multihead_attn(X) + X)\n",
    "        \n",
    "        X = self.norm2(self.dense2(self.dense1(X)) + X)\n",
    "        \n",
    "        if not mask is None:\n",
    "            \n",
    "            X = tf.multiply(X, tf.expand_dims(tf.dtypes.cast(mask, 'float32'), -1))\n",
    "        \n",
    "        return X      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Layer\n",
    "\n",
    "Inputs = D, output from prev decoder layer\n",
    "         E, output from encoder stack\n",
    "         \n",
    "1. $X = Normalize(Multihead(D) + D)$\n",
    "2. $X = Normalize(Multihead(E,X) + X)$\n",
    "3. $X = Normalize(W2\\cdot relu(W1\\cdot X + b1) + b2 + X)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderNode(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, h = 8):\n",
    "        super().__init__(**kwargs)\n",
    "        self.h = h\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        assert(type(input_shape) == list and len(input_shape) == 2),'Expected input as \"[encoder_seq, decoder_seq]\"'\n",
    "        assert(input_shape[0] == input_shape[1]), 'Expected encoder and decoder inputs to have same dimensions'\n",
    "        \n",
    "        (m, Tx, model_dim) = input_shape[0]\n",
    "        \n",
    "        self.masked_attn = MultiHeadAttn(self.h, encoder = False)\n",
    "        self.merged_attn = EncoderDecoderMultiHead(self.h)\n",
    "        self.norm1 = tf.keras.layers.BatchNormalization()\n",
    "        self.norm2 = tf.keras.layers.BatchNormalization()\n",
    "        self.norm3 = tf.keras.layers.BatchNormalization()\n",
    "        self.dense1 = tf.keras.layers.Dense(model_dim, activation = 'relu', use_bias = True)\n",
    "        self.dense2 = tf.keras.layers.Dense(model_dim, activation = 'linear', use_bias = True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        \n",
    "        (encoder_in, decoder_in) = inputs\n",
    "        \n",
    "        X = self.norm1(self.masked_attn(decoder_in) + decoder_in)\n",
    "        \n",
    "        X = self.norm2(self.merged_attn([encoder_in, X]) + X)\n",
    "        \n",
    "        X = self.norm3(self.dense2(self.dense(1)) + X)\n",
    "        \n",
    "        if not mask is None:\n",
    "            \n",
    "            X = tf.multiply(X, tf.expand_dims(tf.dtypes.cast(mask, 'float32'), -1))\n",
    "        \n",
    "        return X  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
