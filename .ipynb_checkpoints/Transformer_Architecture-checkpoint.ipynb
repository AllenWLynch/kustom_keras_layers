{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaled Dot-Product attention function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product_attn(q, k, v, mask = None):\n",
    "    \n",
    "    lenQ = q.get_shape()[-1]\n",
    "    \n",
    "    energies = tf.multiply(1/lenQ**0.5, tf.matmul(q, k, transpose_b = True))\n",
    "    \n",
    "    if not mask is None:\n",
    "        mask = (1. - mask) * -1e9\n",
    "        energies = tf.add(energies, mask)\n",
    "    \n",
    "    alphas = tf.nn.softmax(energies, axis = -1)\n",
    "    \n",
    "    context = tf.matmul(alphas, v)\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, k, d_mod = 2, 5, 12\n",
    "Q, K, V = tf.random.uniform((m, k, d_mod)), tf.random.uniform((m, k, d_mod)), tf.random.uniform((m, k, d_mod))\n",
    "\n",
    "m = tf.convert_to_tensor(np.array([[1,1,0,0,0],[1,1,1,1,0]]), dtype = 'float32')\n",
    "m = m[:, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 5, 12])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_product_attn(Q, K, V, mask = m).get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multihead Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadProjection(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, projected_dim, heads = 8, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.h = heads\n",
    "        self.projected_dim = projected_dim\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "         \n",
    "        assert(len(input_shape) == 3), 'Expected input of rank 3: (m, Tx, d_model)'\n",
    "        \n",
    "        self.m, self.k, self.model_dim = input_shape\n",
    "        \n",
    "        self.W = self.add_weight(\n",
    "                shape = (self.h, self.model_dim, self.projected_dim), \n",
    "                initializer = 'glorot_normal', \n",
    "                trainable = True)\n",
    "        \n",
    "        self.b = self.add_weight(shape = (self.h, 1, self.projected_dim), initializer = 'Zeros', \n",
    "                trainable = True)\n",
    "        \n",
    "    def call(self, X):\n",
    "        \n",
    "        X = tf.expand_dims(X, 1) # adds a head layer\n",
    "        \n",
    "        output = tf.add(tf.matmul(X, self.W), self.b)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 5, 64)\n"
     ]
    }
   ],
   "source": [
    "projer = MultiHeadProjection(8, 8)\n",
    "x = tf.random.normal((2, 5, 64))\n",
    "print(x.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 8, 5, 8])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projer(x).get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder Attn Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "   \n",
    "    def __init__(self, projected_dim, heads = 8, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.h = heads\n",
    "        self.projected_dim = projected_dim\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        for input_ in input_shape:\n",
    "            assert(len(input_) == 3), 'Expected input shape of (m, Tx, d)'\n",
    "        \n",
    "        (self.projQ, self.projK, self.projV) = (MultiHeadProjection(self.projected_dim, self.h) \n",
    "                                       for input_ in input_shape)\n",
    "        \n",
    "        (output_m, output_k, output_d) = input_shape[-1]\n",
    "        \n",
    "        self.reshaper = tf.keras.layers.Reshape(target_shape = (-1, self.projected_dim * self.h))\n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(output_d)\n",
    "        \n",
    "    def call(self, X, mask = None):\n",
    "        '''\n",
    "        Arguments\n",
    "        X: list of (Q, K, V)\n",
    "        mask: for softmax layer\n",
    "        '''\n",
    "        \n",
    "        (Q,K,V) = X\n",
    "        \n",
    "        Q, K, V = self.projQ(Q), self.projK(K), self.projV(V)\n",
    "        \n",
    "        #print(Q.get_shape(), K.get_shape(), V.get_shape())\n",
    "        \n",
    "        attention = dot_product_attn(Q, K, V, mask = mask)\n",
    "        \n",
    "        #print(attention.get_shape())\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        flattened = self.reshaper(attention)\n",
    "        \n",
    "        #print(flattened.get_shape())\n",
    "        \n",
    "        output = self.dense(flattened)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 5, 64)\n"
     ]
    }
   ],
   "source": [
    "attn = AttentionLayer(8, 8)\n",
    "x = tf.random.normal((2, 5, 64))\n",
    "print(x.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 5, 64])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn((x,x,x)).get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNNLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, d_model, dff, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_model, self.dff = d_model, dff\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.dense1 = tf.keras.layers.Dense(self.dff, activation = 'relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(self.d_model, activation = 'linear')\n",
    "        \n",
    "    def call(self, X):\n",
    "        return self.dense2(self.dense1(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.normal((2, 5, 512))\n",
    "f = FCNNLayer(512, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 5, 512])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = f(x)\n",
    "x.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, dff = 2048, heads = 8, dropout = 0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.h = heads\n",
    "        self.dropout = dropout\n",
    "        self.dff = dff\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        assert(len(input_shape) == 3), 'Expected input shape of (m, Tx, d)'\n",
    "        \n",
    "        (self.m, self.k, self.d_model) = input_shape\n",
    "        \n",
    "        self.projected_dim = self.d_model//self.h\n",
    "        \n",
    "        self.attn = AttentionLayer(self.projected_dim, self.h)\n",
    "        self.drop1 = tf.keras.layers.Dropout(self.dropout)\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization()\n",
    "        \n",
    "        self.fcnn = FCNNLayer(self.d_model, self.dff)\n",
    "        self.drop2 = tf.keras.layers.Dropout(self.dropout)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization()\n",
    "                \n",
    "    def call(self, X, training = True, mask = None):\n",
    "        \n",
    "        attn_output = self.drop1(self.attn([X,X,X], mask = mask), training = training)\n",
    "        \n",
    "        X = self.norm1(attn_output + X)\n",
    "        \n",
    "        fcnn_output = self.drop2(self.fcnn(X), training = training)\n",
    "        \n",
    "        X = self.norm2(fcnn_output + X)\n",
    "        \n",
    "        return X  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.normal((2, 5, 512))\n",
    "encoder_layer = TransformerEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 5, 512])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = encoder_layer(x)\n",
    "y.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, dff = 2048, heads = 8, dropout = 0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.h = heads\n",
    "        self.dropout = dropout\n",
    "        self.dff = dff\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        assert(len(input_shape) == 3), 'Expected input shape of (m, Tx, d)'\n",
    "        \n",
    "        (self.m, self.k, self.d_model) = input_shape\n",
    "        \n",
    "        self.projected_dim = self.d_model//self.h\n",
    "        \n",
    "        self.intr_attn = AttentionLayer(self.projected_dim, self.h)\n",
    "        self.drop1 = tf.keras.layers.Dropout(self.dropout)\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization()\n",
    "        \n",
    "        self.enc_dec_attn = AttentionLayer(self.projected_dim, self.h)\n",
    "        self.drop2 = tf.keras.layers.Dropout(self.dropout)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization()\n",
    "        \n",
    "        self.fcnn = FCNNLayer(self.d_model, self.dff)\n",
    "        self.drop3 = tf.keras.layers.Dropout(self.dropout)\n",
    "        self.norm3 = tf.keras.layers.LayerNormalization()\n",
    "                \n",
    "    def call(self, X, encoder_output, lookahead_mask = None, encoder_padding_mask = None, training = True):\n",
    "        \n",
    "        # attention mechanism 1\n",
    "        attn_output = self.drop1(self.intr_attn([X,X,X], mask = lookahead_mask), training = training)\n",
    "        X = self.norm1(attn_output + X)\n",
    "                              \n",
    "        # attention mechanism 2\n",
    "        attn_output = self.drop2(self.enc_dec_attn([X,encoder_output, encoder_output], mask = encoder_padding_mask), training = training)\n",
    "        X = self.norm2(attn_output + X)\n",
    "                                 \n",
    "        # fcnn\n",
    "        fcnn_output = self.drop3(self.fcnn(X), training = training)\n",
    "        X = self.norm3(fcnn_output + X)               \n",
    "                \n",
    "        return X  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.normal((2, 5, 512))\n",
    "decoder_layer = TransformerDecoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 5, 512])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_layer(x, y).get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Position Encoding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        (self.m, self.k, self.d_model) = input_shape\n",
    "        \n",
    "        pos = np.arange(self.k).reshape(-1,1)\n",
    "        \n",
    "        i = 1 / np.power(10000, 2 * np.arange(self.d_model) / self.d_model)\n",
    "        \n",
    "        embeddings = pos * i\n",
    "        \n",
    "        evens = np.arange(0, self.d_model, 2)\n",
    "        \n",
    "        odds = evens + 1\n",
    "        \n",
    "        embeddings[:, evens] = np.sin(embeddings[:, evens])\n",
    "        \n",
    "        embeddings[:, odds] = np.cos(embeddings[:, odds])\n",
    "        \n",
    "        self.embeddings = tf.convert_to_tensor(np.expand_dims(embeddings, 0), dtype = 'float32')\n",
    "        \n",
    "    def call(self, X):\n",
    "        X = X + self.embeddings\n",
    "        \n",
    "        return tf.multiply(X, self.d_model**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = PositionalEmbedding()\n",
    "x = tf.ones((3, 5, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 5, 12])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pe(x)\n",
    "y.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderStack(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, num_classes, d_model = 512, num_layers = 6, num_heads = 8, dropout = 0.1, dff = 2048, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.dff = dff\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "                \n",
    "        (m, k) = input_shape\n",
    "        \n",
    "        seq_shape = (m, k, self.d_model)\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(self.num_classes, self.d_model, mask_zero = True)\n",
    "        self.positional_embedding = PositionalEmbedding()\n",
    "        \n",
    "        self.encoders = [\n",
    "            TransformerEncoder(dff = self.dff, heads = self.num_heads, dropout = self.dropout) \n",
    "            for i in range(self.num_layers)\n",
    "        ]\n",
    "        \n",
    "    def call(self, seqs, training = True):\n",
    "        \n",
    "        X = self.embedding(seqs)\n",
    "        \n",
    "        #expand the mask from the embedding layer from (m, Tx) to (m, 1, 1, Tx) for multihead softmax\n",
    "        encoder_mask = tf.dtypes.cast(self.embedding.compute_mask(seqs), 'float32')[:, tf.newaxis, tf.newaxis, :]\n",
    "        \n",
    "        X = self.positional_embedding(X)\n",
    "        \n",
    "        #call(self, X, encoder_output, lookahead_mask = None, encoder_padding_mask = None, training = True)\n",
    "        for encoder in self.encoders:\n",
    "            X = encoder(X, mask = encoder_mask, training = training)\n",
    "            \n",
    "        return X, encoder_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderStack(tf.keras.layers.Layer):\n",
    "    \n",
    "    \n",
    "    def __init__(self, num_classes, d_model = 512, num_layers = 6, num_heads = 8, dropout = 0.1, dff = 2048, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.dff = dff\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        assert(len(input_shape) == 3), 'Expected input with len 3 in the form of (decoder_input, encoder_output, encoder_mask)'\n",
    "        assert(input_shape[0][1] == input_shape[1][1]), 'Expected encoder output and decoder input to have same time dimension'\n",
    "        \n",
    "        (m, k) = input_shape[0]\n",
    "        \n",
    "        num_ones = 0.5 * (k**2 + k)\n",
    "\n",
    "        self.trailing_mask = tfp.math.fill_triangular(tf.ones(num_ones), upper = False)\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(self.num_classes, self.d_model, mask_zero = True)\n",
    "        \n",
    "        self.positional_embedding = PositionalEmbedding()\n",
    "        \n",
    "        self.decoders = [\n",
    "            TransformerDecoder(dff = self.dff, heads = self.num_heads, dropout = self.dropout) \n",
    "            for i in range(self.num_layers)\n",
    "        ]\n",
    "            \n",
    "    def call(self, inputs, training = True):\n",
    "        \n",
    "        (seqs, encoder_output, encoder_mask) = inputs\n",
    "        \n",
    "        X = self.embedding(seqs)\n",
    "        \n",
    "        #expand the mask from the embedding layer from (m, Tx) to (m, 1, 1, Tx) for multihead softmax\n",
    "        decoder_padding_mask = tf.dtypes.cast(self.embedding.compute_mask(seqs), 'float32')[:, tf.newaxis, tf.newaxis, :]\n",
    "        #then add trailing mask to it\n",
    "        decoder_mask = tf.multiply(decoder_padding_mask, self.trailing_mask)\n",
    "        \n",
    "        #print(decoder_mask)\n",
    "        \n",
    "        X = self.positional_embedding(X)\n",
    "        \n",
    "        #call(self, X, encoder_output, lookahead_mask = None, encoder_padding_mask = None, training = True)\n",
    "        for decoder in self.decoders:\n",
    "            X = decoder(X, encoder_output, lookahead_mask = decoder_mask, \n",
    "                        encoder_padding_mask = encoder_mask, training = training)\n",
    "            \n",
    "        X = tf.matmul(X, tf.transpose(self.embedding.embeddings))\n",
    "            \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 7)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 2, 3, 4, 0, 0, 0],[2,3,4,0,0,0,0],[1,5,0,0,0,0,0]])\n",
    "print(x.shape)\n",
    "\n",
    "encode = EncoderStack(100)\n",
    "x, m = encode(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 7, 100])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([[1, 2, 3, 4, 0, 0, 0],[2,3,4,0,0,0,0],[1,5,0,0,0,0,0]])\n",
    "decode = DecoderStack(100)\n",
    "\n",
    "y = decode((y, x, m))\n",
    "y.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Transformer(num_classes, max_seq_len, d_model = 512, num_layers = 6, num_heads = 8, dropout = 0.1, dff = 2048):\n",
    "        \n",
    "        X = tf.keras.Input(shape = (max_seq_len,))\n",
    "        \n",
    "        Y = tf.keras.Input(shape = (max_seq_len,))\n",
    "        \n",
    "        enc_output, encoder_mask = EncoderStack(num_classes, d_model, num_layers, num_heads, dropout, dff)(X)\n",
    "    \n",
    "        logits = DecoderStack(num_classes, d_model, num_layers, num_heads, dropout, dff)((Y, enc_output, encoder_mask))\n",
    "        \n",
    "        return tf.keras.Model(inputs = [X,Y], outputs = [logits])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1, 2, 3, 4, 0, 0, 0],[2,3,4,0,0,0,0],[1,5,0,0,0,0,0]])\n",
    "y = np.array([[1, 2, 3, 4, 0, 0, 0],[2,3,4,0,0,0,0],[1,5,0,0,0,0,0]])\n",
    "\n",
    "tr = Transformer(10, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 7, 10)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.predict([x,y]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLoss():\n",
    "    \n",
    "    '''\n",
    "    logits: output from linear dense layer (m, Tx, num_classes)\n",
    "    y_true: labels for sparse crossentropy (m, Tx)\n",
    "    epsilon: label smoothing factor [0,1]\n",
    "    returns loss value\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, epsilon = 0.):\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def __call__(self, labels, logits):\n",
    "    \n",
    "        (m, Tx, num_classes) = logits.get_shape()\n",
    "\n",
    "        expanded_y = tf.keras.utils.to_categorical(labels, num_classes = num_classes) #shape = (m, Tx, num_classes)\n",
    "\n",
    "        losses = tf.keras.losses.categorical_crossentropy(expanded_y, logits, from_logits=True, label_smoothing = self.epsilon)\n",
    "\n",
    "        mask = 1. - tf.dtypes.cast(tf.math.equal(y_true, 0), 'float32')\n",
    "\n",
    "        return tf.reduce_mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([2, 5, 12]), (2, 5))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = tf.random.normal((2,5,12))\n",
    "\n",
    "y_true = np.array([[1,2,3,0,0],[2,2,3,3,0]])\n",
    "\n",
    "logits.shape, y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=16242, shape=(), dtype=float32, numpy=3.642079>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losser = TransformerLoss(epsilon = 0.1)\n",
    "losser(y_true, logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TransformerOptimizer(d_model):\n",
    "    \n",
    "    learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "    return tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
